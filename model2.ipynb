{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_url = \"https://tfhub.dev/tensorflow/movinet/a0/stream/kinetics-600/classification/3\"\n",
    "\n",
    "encoder = hub.KerasLayer(hub_url, trainable=True)\n",
    "\n",
    "# Define the image (video) input\n",
    "image_input = tf.keras.layers.Input(\n",
    "    shape=[None, None, None, 3],\n",
    "    dtype=tf.float32,\n",
    "    name='image')\n",
    "\n",
    "# Define the state inputs, which is a dict that maps state names to tensors.\n",
    "init_states_fn = encoder.resolved_object.signatures['init_states']\n",
    "state_shapes = {\n",
    "    name: ([s if s > 0 else None for s in state.shape], state.dtype)\n",
    "    for name, state in init_states_fn(tf.constant([0, 0, 0, 0, 3])).items()\n",
    "}\n",
    "states_input = {\n",
    "    name: tf.keras.Input(shape[1:], dtype=dtype, name=name)\n",
    "    for name, (shape, dtype) in state_shapes.items()\n",
    "}\n",
    "\n",
    "# The inputs to the model are the states and the video\n",
    "inputs = {**states_input, 'image': image_input}\n",
    "\n",
    "outputs = encoder(inputs)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name='movinet')\n",
    "\n",
    "# Create your example input here.\n",
    "# Refer to the description or paper for recommended input shapes.\n",
    "example_input = tf.ones([1, 8, 172, 172, 3])\n",
    "\n",
    "# Split the video into individual frames.\n",
    "# Note: we can also split into larger clips as well (e.g., 8-frame clips).\n",
    "# Running on larger clips will slightly reduce latency overhead, but\n",
    "# will consume more memory.\n",
    "frames = tf.split(example_input, example_input.shape[1], axis=1)\n",
    "\n",
    "# Initialize the dict of states. All state tensors are initially zeros.\n",
    "init_states = init_states_fn(tf.shape(example_input))\n",
    "\n",
    "# Run the model prediction by looping over each frame.\n",
    "states = init_states\n",
    "predictions = []\n",
    "for frame in frames:\n",
    "    output, states = model({**states, 'image': frame})\n",
    "    predictions.append(output)\n",
    "\n",
    "# The video classification will simply be the last output of the model.\n",
    "final_prediction = tf.argmax(predictions[-1], -1)\n",
    "\n",
    "# Alternatively, we can run the network on the entire input video.\n",
    "# The output should be effectively the same\n",
    "# (but it may differ a small amount due to floating point errors).\n",
    "non_streaming_output, _ = model({**init_states, 'image': example_input})\n",
    "non_streaming_prediction = tf.argmax(non_streaming_output, -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc76d56709ed3aec67cb600f3a9ef36c8619eecd51ebe5e906813d43ccd3ce5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
